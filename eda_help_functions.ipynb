{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvs6epSG79juoI9wai327a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inbalv/tictactoe/blob/master/eda_help_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my_lEfBCnOWs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA"
      ],
      "metadata": {
        "id": "eKf6exzNq3a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_histograms_by_label(df, numeric_cols, figsize=(10,25)):\n",
        "    \"\"\"\n",
        "    Plots histograms for each numeric column in the DataFrame, differentiated by the 'label' column.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pandas DataFrame containing the data.\n",
        "    - numeric_cols: list of column names (str) representing the numeric columns to plot.\n",
        "    - nrows: number of rows in the subplot grid.\n",
        "    - ncols: number of columns in the subplot grid.\n",
        "    - figsize: tuple defining the overall figure size.\n",
        "    \"\"\"\n",
        "    # Create the subplot grid\n",
        "    fig, axes = plt.subplots(nrows= len(numeric_cols)+1, ncols=2, figsize=figsize)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Loop through each numeric column and create a histogram\n",
        "    for i, col in enumerate(numeric_cols):\n",
        "        if i < len(axes):\n",
        "            ax = axes[i]\n",
        "            sns.histplot(x=col, hue='label', data=df, ax=ax, log_scale=True,\n",
        "                         element=\"step\", fill=False)\n",
        "            ax.set_title(f\"Collisions Outcome by {col}\")\n",
        "            ax.set_xlabel(col)\n",
        "            ax.set_ylabel(\"Count\")\n",
        "\n",
        "    # Hide any unused subplots if there are more axes than numeric columns\n",
        "    for j in range(len(numeric_cols), len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "E4tjDlgAqSSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_monthly_feature_averages_with_error(df, month_col='month', numeric_cols=None, date_format='%b-%y'):\n",
        "    \"\"\"\n",
        "    Plots the monthly average of selected numeric features with standard error bars.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pandas DataFrame containing the data.\n",
        "    - month_col: the name of the column containing month information (default 'month').\n",
        "    - numeric_cols: list of numeric column names to plot. Must be provided.\n",
        "    - date_format: datetime format to parse the month column (default '%b-%y').\n",
        "\n",
        "    This function:\n",
        "      1. Converts the month column into a datetime object and sorts the DataFrame.\n",
        "      2. For each numeric column, calculates the monthly average and standard error.\n",
        "      3. Plots each feature on a separate subplot with error bars.\n",
        "    \"\"\"\n",
        "    if numeric_cols is None:\n",
        "        raise ValueError(\"Please provide a list of numeric columns to examine.\")\n",
        "\n",
        "    # Convert the month column to datetime and sort the data chronologically.\n",
        "    df['month_date'] = pd.to_datetime(df[month_col], format=date_format, errors='coerce')\n",
        "    df = df.sort_values('month_date')\n",
        "\n",
        "    # Set up the subplots: one row per feature.\n",
        "    num_features = len(numeric_cols)\n",
        "    fig, axs = plt.subplots(num_features, 1, figsize=(12, 6 * num_features))\n",
        "\n",
        "    # Ensure axs is iterable even if there's only one subplot.\n",
        "    if num_features == 1:\n",
        "        axs = [axs]\n",
        "\n",
        "    # Loop through each feature, calculate the monthly mean and standard error, then plot.\n",
        "    for i, feat in enumerate(numeric_cols):\n",
        "        grouped = df.groupby('month_date')[feat]\n",
        "        monthly_mean = grouped.mean()\n",
        "        monthly_std = grouped.std()\n",
        "        monthly_count = grouped.count()\n",
        "        monthly_se = monthly_std / np.sqrt(monthly_count)\n",
        "\n",
        "        axs[i].errorbar(\n",
        "            monthly_mean.index,\n",
        "            monthly_mean.values,\n",
        "            yerr=monthly_se.values,\n",
        "            marker='o',\n",
        "            linestyle='-',\n",
        "            capsize=5\n",
        "        )\n",
        "        axs[i].set_title(f\"Average {feat} Over Time\")\n",
        "        axs[i].set_xlabel(\"Month\")\n",
        "        axs[i].set_ylabel(f\"Average {feat}\")\n",
        "        axs[i].grid(True)\n",
        "        axs[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "1eUOAlMZqS89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
        "from xgboost import XGBClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "\n",
        "def print_formatted_metrics(y_test, y_pred):\n",
        "    \"\"\"\n",
        "    Print a nicely formatted confusion matrix and classification report.\n",
        "\n",
        "    Args:\n",
        "        y_test: Array-like of true labels.\n",
        "        y_pred: Array-like of predicted labels.\n",
        "    \"\"\"\n",
        "    # Create confusion matrix DataFrame with labels.\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    cm_df = pd.DataFrame(cm,\n",
        "                         index=[\"Actual Negative\", \"Actual Positive\"],\n",
        "                         columns=[\"Predicted Negative\", \"Predicted Positive\"])\n",
        "\n",
        "    # Print the confusion matrix using tabulate for prettier formatting.\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(tabulate(cm_df, headers=\"keys\", tablefmt=\"psql\"))\n",
        "\n",
        "    # Create the classification report as a DataFrame.\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "\n",
        "    # Print the classification report using tabulate.\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(tabulate(report_df, headers=\"keys\", tablefmt=\"psql\"))\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate the trained model on the test data using AUC-PR as the main metric.\n",
        "\n",
        "    This function:\n",
        "      - Computes predictions and predicted probabilities.\n",
        "      - Calculates ROC AUC and Average Precision (AUC-PR).\n",
        "      - Prints the metrics, confusion matrix, and classification report.\n",
        "      - Plots the Precision-Recall Curve.\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    avg_precision = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "    print(\"ROC AUC Score: {:.4f}\".format(roc_auc))\n",
        "    print(\"Average\n"
      ],
      "metadata": {
        "id": "D-t9kPVqwLlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
        "from xgboost import XGBClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "\n",
        "def print_formatted_metrics(y_test, y_pred):\n",
        "    \"\"\"\n",
        "    Print a nicely formatted confusion matrix and classification report.\n",
        "\n",
        "    Args:\n",
        "        y_test: Array-like of true labels.\n",
        "        y_pred: Array-like of predicted labels.\n",
        "    \"\"\"\n",
        "    # Create confusion matrix DataFrame with labels.\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    cm_df = pd.DataFrame(cm,\n",
        "                         index=[\"Actual Negative\", \"Actual Positive\"],\n",
        "                         columns=[\"Predicted Negative\", \"Predicted Positive\"])\n",
        "\n",
        "    # Print the confusion matrix using tabulate for prettier formatting.\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(tabulate(cm_df, headers=\"keys\", tablefmt=\"psql\"))\n",
        "\n",
        "    # Create the classification report as a DataFrame.\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "\n",
        "    # Print the classification report using tabulate.\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(tabulate(report_df, headers=\"keys\", tablefmt=\"psql\"))\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate the trained model on the test data using AUC-PR as the main metric.\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    avg_precision = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "    print(\"ROC AUC Score: {:.4f}\".format(roc_auc))\n",
        "    print(\"Average Precision (AUC-PR): {:.4f}\".format(avg_precision))\n",
        "    #print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "    #print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "    print_formatted_metrics(y_test, y_pred)\n",
        "    # Plot Precision-Recall Curve\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(recall, precision, marker='.', label='Precision-Recall curve (AUC = {:.4f})'.format(pr_auc))\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "I392Ii9dqTPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_feature_importance(model, importance_type='gain', top_n=40, figsize=(20,10)):\n",
        "    \"\"\"\n",
        "    Plots the top N features based on the specified importance type from an XGBoost model.\n",
        "\n",
        "    Parameters:\n",
        "        model: The trained XGBoost model.\n",
        "        importance_type (str): The type of feature importance to use ('gain', 'weight', etc.). Default is 'gain'.\n",
        "        top_n (int): Number of top features to plot. Default is 40.\n",
        "        figsize (tuple): Size of the figure for the plot. Default is (20, 10).\n",
        "    \"\"\"\n",
        "    # Retrieve feature importance scores from the booster.\n",
        "    feature_importance = model.get_booster().get_score(importance_type=importance_type)\n",
        "\n",
        "    # Extract keys (feature names) and values (importance scores)\n",
        "    keys = list(feature_importance.keys())\n",
        "    values = list(feature_importance.values())\n",
        "\n",
        "    # Create a DataFrame, sort it, and plot the top features.\n",
        "    data = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by=\"score\", ascending=False)\n",
        "    data.nlargest(top_n, columns=\"score\").plot(kind='barh', figsize=figsize)\n",
        "\n",
        "    plt.title(f'Top {top_n} Features by {importance_type.capitalize()} Importance')\n",
        "    plt.xlabel(\"Importance Score\")\n",
        "    plt.ylabel(\"Features\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "rwCr6HvFqT2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def missing_and_distinct_stats(df):\n",
        "    total_count = len(df)\n",
        "    distinct_counts = df.nunique(dropna=True)\n",
        "    missing_counts = df.isnull().sum()\n",
        "\n",
        "    summary = pd.DataFrame({\n",
        "        'Distinct Count': distinct_counts,\n",
        "        'Missing Count': missing_counts,\n",
        "        'Missing%': (missing_counts / total_count) * 100,\n",
        "        'Data Type': df.dtypes\n",
        "    })\n",
        "\n",
        "    display(summary)\n",
        "\n",
        "    summary_stats = df.describe()\n",
        "    display(summary_stats.T)\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "L2vJHcsGBqRf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}