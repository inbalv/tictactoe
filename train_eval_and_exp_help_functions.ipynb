{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNf4YSlOBccnfP5h5iE1LB1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inbalv/tictactoe/blob/master/train_eval_and_exp_help_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irSNxqU0HEhd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wL4g7nwUHRqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperopt\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "import numpy as np\n",
        "from hyperopt.pyll.base import scope\n",
        "from hyperopt.pyll.stochastic import sample\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "# Returns an integer from 0 to 10, inclusive\n",
        "sample(scope.int(hp.quniform('example', 0, 10, 1)))\n",
        "\n",
        "# Objective function for XGBoost\n",
        "def objective_xgb(params):\n",
        "    params['n_estimators'] = 1000\n",
        "\n",
        "    print(\"Training with params: {}\".format(params))\n",
        "    clf = xgb.XGBClassifier(\n",
        "        objective='binary:logistic',\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss',\n",
        "        **params\n",
        "    )\n",
        "\n",
        "    clf.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        early_stopping_rounds=25,\n",
        "        verbose=True  # Set to True to see evaluation messages\n",
        "    )\n",
        "\n",
        "    pred_proba = clf.predict_proba(X_val)[:, 1]\n",
        "    auc = roc_auc_score(y_val, pred_proba)\n",
        "    return {'loss': -auc, 'status': STATUS_OK}\n",
        "\n",
        "# Hyperparameter search space (note: 'min_data_in_leaf' has been removed)\n",
        "space = {\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 4, 10, 1)),\n",
        "    'learning_rate': hp.quniform('learning_rate', 0.1, 0.4, 0.05),\n",
        "    'reg_alpha': hp.quniform('reg_alpha', 0, 5, 0.1),\n",
        "    'reg_lambda': hp.quniform('reg_lambda', 0, 5, 0.1),\n",
        "    'gamma': hp.quniform('gamma', 0, 5, 0.1),\n",
        "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
        "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 0.5)\n",
        "}\n",
        "\n",
        "trials = Trials()\n",
        "best = fmin(fn=objective_xgb, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
        "\n",
        "print(\"Best hyperparameters:\", best)\n",
        "\n",
        "# Train the final model using the best hyperparameters\n",
        "best_clf = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    **best,\n",
        "    n_estimators=1000,\n",
        "    random_state=42,\n",
        "    verbosity=0  # Set to 0 to suppress detailed logs during final training\n",
        ")\n",
        "\n",
        "best_clf.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    early_stopping_rounds=25,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "y_test_pred_proba = best_clf.predict_proba(X_test)[:, 1]\n",
        "y_test_pred = (y_test_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "accuracy_xgb = accuracy_score(y_test, y_test_pred)\n",
        "precision_xgb = precision_score(y_test, y_test_pred)\n",
        "recall_xgb = recall_score(y_test, y_test_pred)\n",
        "roc_auc_xgb = roc_auc_score(y_test, y_test_pred_proba)\n",
        "pr_auc = average_precision_score(y_test, y_test_pred_proba)\n",
        "\n",
        "print(\"XGBoost (fit) metrics:\")\n",
        "print(\"Accuracy:\", accuracy_xgb)\n",
        "print(\"Precision:\", precision_xgb)\n",
        "print(\"Recall:\", recall_xgb)\n",
        "print(\"ROC AUC:\", roc_auc_xgb)\n",
        "print(\"PR AUC:\", pr_auc)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "IYDf6WXcHR5M",
        "outputId": "f3abd599-f8ff-4912-84ca-609f93524fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with params: {'colsample_bytree': 0.9502779280110689, 'gamma': 2.0, 'learning_rate': 0.35000000000000003, 'max_depth': 9, 'min_child_weight': 7.0, 'reg_alpha': 4.2, 'reg_lambda': 1.6, 'subsample': 0.5256585540467105, 'n_estimators': 1000}\n",
            "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:hyperopt.fmin:job exception: name 'X_train' is not defined\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-858dfed3cdd3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobjective_xgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best hyperparameters:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mallow_trials_fmin\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fmin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         return trials.fmin(\n\u001b[0m\u001b[1;32m    541\u001b[0m             \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfmin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         return fmin(\n\u001b[0m\u001b[1;32m    672\u001b[0m             \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;31m# next line is where the fmin is actually executed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                     \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job exception: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             )\n\u001b[0;32m--> 892\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-858dfed3cdd3>\u001b[0m in \u001b[0;36mobjective_xgb\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     clf.fit(\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
        "from xgboost import XGBClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "\n",
        "def print_formatted_metrics(y_test, y_pred):\n",
        "    \"\"\"\n",
        "    Print a nicely formatted confusion matrix and classification report.\n",
        "\n",
        "    Args:\n",
        "        y_test: Array-like of true labels.\n",
        "        y_pred: Array-like of predicted labels.\n",
        "    \"\"\"\n",
        "    # Create confusion matrix DataFrame with labels.\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    cm_df = pd.DataFrame(cm,\n",
        "                         index=[\"Actual Negative\", \"Actual Positive\"],\n",
        "                         columns=[\"Predicted Negative\", \"Predicted Positive\"])\n",
        "\n",
        "    # Print the confusion matrix using tabulate for prettier formatting.\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(tabulate(cm_df, headers=\"keys\", tablefmt=\"psql\"))\n",
        "\n",
        "    # Create the classification report as a DataFrame.\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "\n",
        "    # Print the classification report using tabulate.\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(tabulate(report_df, headers=\"keys\", tablefmt=\"psql\"))\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate the trained model on the test data using AUC-PR as the main metric.\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    avg_precision = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "    print(\"ROC AUC Score: {:.4f}\".format(roc_auc))\n",
        "    print(\"Average Precision (AUC-PR): {:.4f}\".format(avg_precision))\n",
        "    #print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "    #print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "    print_formatted_metrics(y_test, y_pred)\n",
        "    # Plot Precision-Recall Curve\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(recall, precision, marker='.', label='Precision-Recall curve (AUC = {:.4f})'.format(pr_auc))\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "AeGqVgjHHe9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_feature_importance(model, importance_type='gain', top_n=40, figsize=(20,10)):\n",
        "    \"\"\"\n",
        "    Plots the top N features based on the specified importance type from an XGBoost model.\n",
        "\n",
        "    Parameters:\n",
        "        model: The trained XGBoost model.\n",
        "        importance_type (str): The type of feature importance to use ('gain', 'weight', etc.). Default is 'gain'.\n",
        "        top_n (int): Number of top features to plot. Default is 40.\n",
        "        figsize (tuple): Size of the figure for the plot. Default is (20, 10).\n",
        "    \"\"\"\n",
        "    # Retrieve feature importance scores from the booster.\n",
        "    feature_importance = model.get_booster().get_score(importance_type=importance_type)\n",
        "\n",
        "    # Extract keys (feature names) and values (importance scores)\n",
        "    keys = list(feature_importance.keys())\n",
        "    values = list(feature_importance.values())\n",
        "\n",
        "    # Create a DataFrame, sort it, and plot the top features.\n",
        "    data = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by=\"score\", ascending=False)\n",
        "    data.nlargest(top_n, columns=\"score\").plot(kind='barh', figsize=figsize)\n",
        "\n",
        "    plt.title(f'Top {top_n} Features by {importance_type.capitalize()} Importance')\n",
        "    plt.xlabel(\"Importance Score\")\n",
        "    plt.ylabel(\"Features\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "HkROwU0lHUWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_shap_summary(model, X_test):\n",
        "    \"\"\"\n",
        "    Generates and displays a SHAP summary plot for the provided tree-based model and test dataset.\n",
        "\n",
        "    Parameters:\n",
        "        model: Trained tree-based model (e.g., XGBoost) with a booster containing feature names.\n",
        "        X_test: Test dataset (pandas DataFrame or similar) with the same features as used in training.\n",
        "\n",
        "    The function computes the SHAP values using shap.TreeExplainer and then plots the summary plot.\n",
        "    \"\"\"\n",
        "    # Optional check: verify feature names match if X_test is a DataFrame.\n",
        "    feature_names = model.get_booster().feature_names\n",
        "    if hasattr(X_test, 'columns'):\n",
        "        if not np.array_equal(feature_names, X_test.columns):\n",
        "            print(\"Warning: The model's feature names do not match X_test columns.\")\n",
        "\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "    # Plot the SHAP summary plot.\n",
        "    shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Mw7B3tPsHSLt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}